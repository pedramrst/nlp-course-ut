{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# nltk.download('treebank')\n",
    "# nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:04<00:00, 46.23it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sents = []\n",
    "for file in tqdm(treebank.fileids()):\n",
    "    all_sents.extend(treebank.tagged_sents(file, tagset='universal'))\n",
    "for sent in all_sents:\n",
    "    sent.insert(0, (\"###\", \"START\"))\n",
    "    sent.append((\"&&&\", \"END\"))\n",
    "np.random.seed(42)\n",
    "train_data, data = train_test_split(all_sents, train_size=0.8, shuffle=True)\n",
    "test_data, valid_data = train_test_split(data, train_size=0.75, shuffle=False)\n",
    "all_tags = set()\n",
    "all_words = set()\n",
    "for sent in train_data:\n",
    "    for (word, tag) in sent:\n",
    "        all_tags.add(tag)\n",
    "        all_words.add(word)\n",
    "all_tags = list(all_tags)\n",
    "all_words = list(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate emission probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "default_dict = {word.lower(): epsilon for word in all_words}\n",
    "emission_freq = defaultdict(lambda: default_dict.copy())\n",
    "for sent in train_data:\n",
    "    for (word, tag) in sent:\n",
    "        word = word.lower()\n",
    "        emission_freq[tag][word] += 1\n",
    "\n",
    "default_dict = defaultdict(lambda: epsilon)\n",
    "emission_prob = defaultdict(lambda: default_dict.copy())\n",
    "for tag in emission_freq.keys():\n",
    "    count = sum(emission_freq[tag].values())\n",
    "    for word, freq in emission_freq[tag].items():\n",
    "        emission_prob[tag][word] = freq / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate transition probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dict = {tag: epsilon for tag in all_tags}\n",
    "transition_freq = defaultdict(lambda: default_dict.copy())\n",
    "for sent in train_data:\n",
    "    sent_bigrams = list(nltk.bigrams(sent))\n",
    "    for (word1, tag1), (word2, tag2) in sent_bigrams:\n",
    "        transition_freq[tag1][tag2] += 1\n",
    "\n",
    "\n",
    "transition_prob = defaultdict(dict)\n",
    "for tag1 in transition_freq.keys():\n",
    "    count = sum(transition_freq[tag1].values())\n",
    "    for tag2, freq in transition_freq[tag1].items():\n",
    "        transition_prob[tag1][tag2] = transition_freq[tag1][tag2] / count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = []\n",
    "test_true_tags = []\n",
    "test_pred_tags = []\n",
    "\n",
    "for sent in test_data:\n",
    "    sent_words = []\n",
    "    sent_tags = []\n",
    "    del sent[0]\n",
    "    del sent[-1]\n",
    "    for (word, tag) in sent:\n",
    "        sent_words.append(word.lower())\n",
    "        sent_tags.append(tag)\n",
    "    test_sents.append(sent_words)\n",
    "    test_true_tags.append(sent_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 587/587 [00:11<00:00, 50.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9338354652295305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transition_prob['END'] = {tag: 0 for tag in all_tags}\n",
    "epsilon = 0.00001\n",
    "num_of_tags = len(all_tags)\n",
    "for sent in tqdm(test_sents):\n",
    "    viterbi = np.zeros((num_of_tags, len(sent))) + epsilon\n",
    "    backpointer = np.zeros((num_of_tags, len(sent)))\n",
    "    for i, tag in enumerate(all_tags):\n",
    "        viterbi[i][0] = transition_prob['START'][tag] * emission_prob[tag][sent[0]]\n",
    "    for t in range(1, len(sent)):\n",
    "        for s, current_tag in enumerate(all_tags):\n",
    "            viterbi_row = []\n",
    "            backpointer_row = []\n",
    "            for s_prime, prev_tag in enumerate(all_tags):\n",
    "                viterbi_row.append(viterbi[s_prime][t - 1] * transition_prob[all_tags[s_prime]][all_tags[s]] * emission_prob[all_tags[s]][sent[t]])\n",
    "                backpointer_row.append(viterbi[s_prime][t - 1] * transition_prob[all_tags[s_prime]][all_tags[s]])\n",
    "            viterbi[s][t] = max(viterbi_row)\n",
    "            backpointer[s][t] = np.argmax(backpointer_row)\n",
    "    argmax = np.argmax(viterbi[:, -1])\n",
    "    sent_pred_tags = []\n",
    "    highest_idx = np.argmax(viterbi[:, -1])\n",
    "    sent_pred_tags.insert(0, all_tags[highest_idx])\n",
    "    for i in list(reversed(range(1, len(sent)))):\n",
    "        highest_idx = int(backpointer[highest_idx][i])\n",
    "        sent_pred_tags.insert(0, all_tags[highest_idx])\n",
    "    test_pred_tags.append(sent_pred_tags)\n",
    "\n",
    "\n",
    "correct_preds = 0\n",
    "wrong_preds = 0\n",
    "for true_tags, pred_tags in zip(test_true_tags, test_pred_tags):\n",
    "    for true_tag, pred_tag in zip(true_tags, pred_tags):\n",
    "        if true_tag == pred_tag:\n",
    "            correct_preds += 1\n",
    "        else:\n",
    "            wrong_preds += 1\n",
    "print('accuracy: ', float(correct_preds) / float(correct_preds + wrong_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'NOUN', '.', 'NOUN', 'VERB', '.', 'NUM', 'NUM', 'X', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'NOUN', '.', 'CONJ', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'VERB', 'X', 'ADP', '.', 'NUM', 'NUM', 'X', 'ADP', 'ADJ', 'NUM', 'ADP', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', '.']\n",
      "['ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', '.', 'PRON', 'VERB', '.', 'NUM', 'NUM', 'X', 'ADP', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'VERB', 'NOUN', 'NOUN', 'NOUN', 'NOUN', '.', 'CONJ', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'ADJ', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'VERB', 'X', 'ADP', '.', 'NUM', 'NUM', 'X', 'ADP', 'ADJ', 'NUM', 'ADP', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', '.']\n"
     ]
    }
   ],
   "source": [
    "print(test_true_tags[0])\n",
    "print(test_pred_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'the', 'agency', 'for', 'international', 'development', ',', 'appropriators', 'approved', '$', '200', 'million', '*u*', 'in', 'secondary', 'loan', 'guarantees', 'under', 'an', 'expanded', 'trade', 'credit', 'insurance', 'program', ',', 'and', 'total', 'loan', 'guarantees', 'for', 'the', 'overseas', 'private', 'investment', 'corp.', 'are', 'increased', '*-3', 'by', '$', '40', 'million', '*u*', 'over', 'fiscal', '1989', 'as', 'part', 'of', 'the', 'same', 'poland', 'package', '.']\n"
     ]
    }
   ],
   "source": [
    "print(test_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "0.0001\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "print('appropriators' in all_words)\n",
    "print('overseas' in all_words)\n",
    "print('private' in all_words)\n",
    "print(emission_freq['NOUN']['overseas'])\n",
    "print(emission_freq['NOUN']['private'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbb10bc4ef4492045173ede38a9785d3b54d84b177feb6d6259cb4f4e6bb9227"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
