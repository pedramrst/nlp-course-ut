{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk, tree2conlltags, bigrams\n",
    "from nltk.corpus import treebank\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3914/3914 [00:36<00:00, 107.38it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sents = treebank.tagged_sents()\n",
    "preprocessed_sents = []\n",
    "for sent in tqdm(all_sents):\n",
    "    preprocessed_sent = []\n",
    "    ne = ne_chunk(sent)\n",
    "    bio = tree2conlltags(ne)\n",
    "    preprocessed_sent.append((\"###\", \"START\"))\n",
    "    for (word, _, tag) in bio:\n",
    "        preprocessed_sent.append((word, tag))\n",
    "    preprocessed_sent.append((\"&&&\", \"END\"))\n",
    "    preprocessed_sents.append(preprocessed_sent)\n",
    "np.random.seed(42)\n",
    "train_data, test_data = train_test_split(preprocessed_sents, train_size=0.85, shuffle=True)\n",
    "all_tags = set()\n",
    "all_words = set()\n",
    "for sent in train_data:\n",
    "    for (word, tag) in sent:\n",
    "        all_tags.add(tag)\n",
    "        all_words.add(word)\n",
    "all_tags = list(all_tags)\n",
    "all_words = list(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate emission probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "default_dict = {word.lower(): epsilon for word in all_words}\n",
    "emission_freq = defaultdict(lambda: default_dict.copy())\n",
    "for sent in train_data:\n",
    "    for (word, tag) in sent:\n",
    "        word = word.lower()\n",
    "        emission_freq[tag][word] += 1\n",
    "\n",
    "default_dict = defaultdict(lambda: epsilon)\n",
    "emission_prob = defaultdict(lambda: default_dict.copy())\n",
    "for tag in emission_freq.keys():\n",
    "    count = sum(emission_freq[tag].values())\n",
    "    for word, freq in emission_freq[tag].items():\n",
    "        emission_prob[tag][word] = freq / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate transition probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dict = {tag: epsilon for tag in all_tags}\n",
    "transition_freq = defaultdict(lambda: default_dict.copy())\n",
    "for sent in train_data:\n",
    "    sent_bigrams = list(bigrams(sent))\n",
    "    for (word1, tag1), (word2, tag2) in sent_bigrams:\n",
    "        transition_freq[tag1][tag2] += 1\n",
    "\n",
    "\n",
    "transition_prob = defaultdict(dict)\n",
    "for tag1 in transition_freq.keys():\n",
    "    count = sum(transition_freq[tag1].values())\n",
    "    for tag2, freq in transition_freq[tag1].items():\n",
    "        transition_prob[tag1][tag2] = transition_freq[tag1][tag2] / count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = []\n",
    "test_true_tags = []\n",
    "test_pred_tags = []\n",
    "\n",
    "for sent in test_data:\n",
    "    sent_words = []\n",
    "    sent_tags = []\n",
    "    del sent[0]\n",
    "    del sent[-1]\n",
    "    for (word, tag) in sent:\n",
    "        sent_words.append(word.lower())\n",
    "        sent_tags.append(tag)\n",
    "    test_sents.append(sent_words)\n",
    "    test_true_tags.append(sent_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 588/588 [00:11<00:00, 52.90it/s]\n"
     ]
    }
   ],
   "source": [
    "transition_prob['END'] = {tag: 0 for tag in all_tags}\n",
    "epsilon = 0.00001\n",
    "num_of_tags = len(all_tags)\n",
    "for sent in tqdm(test_sents):\n",
    "    viterbi = np.zeros((num_of_tags, len(sent))) + epsilon\n",
    "    backpointer = np.zeros((num_of_tags, len(sent)))\n",
    "    for i, tag in enumerate(all_tags):\n",
    "        viterbi[i][0] = transition_prob['START'][tag] * emission_prob[tag][sent[0]]\n",
    "    for t in range(1, len(sent)):\n",
    "        for s, current_tag in enumerate(all_tags):\n",
    "            viterbi_row = []\n",
    "            backpointer_row = []\n",
    "            for s_prime, prev_tag in enumerate(all_tags):\n",
    "                viterbi_row.append(viterbi[s_prime][t - 1] * transition_prob[all_tags[s_prime]][all_tags[s]] * emission_prob[all_tags[s]][sent[t]])\n",
    "                backpointer_row.append(viterbi[s_prime][t - 1] * transition_prob[all_tags[s_prime]][all_tags[s]])\n",
    "            viterbi[s][t] = max(viterbi_row)\n",
    "            backpointer[s][t] = np.argmax(backpointer_row)\n",
    "    argmax = np.argmax(viterbi[:, -1])\n",
    "    sent_pred_tags = []\n",
    "    highest_idx = np.argmax(viterbi[:, -1])\n",
    "    sent_pred_tags.insert(0, all_tags[highest_idx])\n",
    "    for i in list(reversed(range(1, len(sent)))):\n",
    "        highest_idx = int(backpointer[highest_idx][i])\n",
    "        sent_pred_tags.insert(0, all_tags[highest_idx])\n",
    "    test_pred_tags.append(sent_pred_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pedram\\anaconda3\\envs\\nn\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Pedram\\anaconda3\\envs\\nn\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "    I-FACILITY       0.89      0.97      0.93       166\n",
      "    B-LOCATION       0.72      0.84      0.78        89\n",
      "      B-PERSON       0.76      0.58      0.66      5960\n",
      "I-ORGANIZATION       0.58      0.58      0.58      2865\n",
      "         I-GPE       0.72      0.93      0.81       555\n",
      "    I-LOCATION       0.74      0.79      0.77        89\n",
      "      I-PERSON       0.58      0.60      0.59      2983\n",
      "         B-GPE       0.82      0.74      0.77      5131\n",
      "             O       0.98      0.99      0.98    228479\n",
      "    B-FACILITY       0.82      0.98      0.89       168\n",
      "         B-GSP       1.00      0.38      0.55        77\n",
      "         I-GSP       0.00      0.00      0.00         2\n",
      "B-ORGANIZATION       0.64      0.56      0.60      4462\n",
      "\n",
      "      accuracy                           0.96    251026\n",
      "     macro avg       0.71      0.69      0.69    251026\n",
      "  weighted avg       0.96      0.96      0.96    251026\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pedram\\anaconda3\\envs\\nn\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "flat_true_tags, flat_pred_tags = [], []\n",
    "for true_tags, pred_tags in zip(test_true_tags, test_pred_tags):\n",
    "    true_temp, pred_temp = [], []\n",
    "    for t1, t2 in zip(true_tags, pred_tags):\n",
    "        if t1 == 'START' or t1 == 'END' or t2 == 'START' or t2 == 'END':\n",
    "            pass\n",
    "        else:\n",
    "            true_temp.append(t1)\n",
    "            pred_temp.append(t2)\n",
    "        flat_true_tags.extend(true_temp)\n",
    "        flat_pred_tags.extend(pred_temp)\n",
    "labels = all_tags.copy()\n",
    "labels.remove('START')\n",
    "labels.remove('END')\n",
    "print(classification_report(flat_true_tags, flat_pred_tags, labels=labels))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbb10bc4ef4492045173ede38a9785d3b54d84b177feb6d6259cb4f4e6bb9227"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
