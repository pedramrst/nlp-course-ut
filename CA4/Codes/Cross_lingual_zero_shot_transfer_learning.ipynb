{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itL9aq7dXRC3",
        "outputId": "13e857ed-da2e-44c1-afba-d6876ea9c8dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.2 MB 8.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 66.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zURjqebbYAX2",
        "outputId": "76fd1bab-23c5-4184-aef5-f2916daca6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUFCySmDYEkn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import AdamW\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "from scipy.special import softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alnAR64pYIMe"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/Colab Notebooks/Multilingual Classification/train.xlsx'\n",
        "test_path = '/content/drive/MyDrive/Colab Notebooks/Multilingual Classification/test.xlsx'\n",
        "valid_path = '/content/drive/MyDrive/Colab Notebooks/Multilingual Classification/valid.xlsx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mrUjTUfXYRa5",
        "outputId": "11fd53f7-61eb-4c76-9a5c-a2bf75cdba5b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lQjmxo6YTOA"
      },
      "outputs": [],
      "source": [
        "def source_preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
        "  return text\n",
        "\n",
        "def target_preprocess(text):\n",
        "  punctuations = string.punctuation + \"؟«»؛،\"\n",
        "  text = text.translate(str.maketrans(punctuations, ' ' * len(punctuations)))\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx0yaDInYVIw"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, dataframe, labels, tokenizer, features, features_preprocess, max_len):\n",
        "        self.dataframe = dataframe\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.features_preprocess = features_preprocess\n",
        "        self.max_len = max_len\n",
        "        self.features = features\n",
        "        \n",
        "    def get_dataset(self):\n",
        "      input_ids = []\n",
        "      attention_masks = []\n",
        "      targets = []\n",
        "      for index, row in self.dataframe.iterrows():\n",
        "        if row['category'] not in self.labels.keys():\n",
        "          continue\n",
        "        target = self.labels[row['category']]\n",
        "        combined_encoded = []\n",
        "        for i, (feature, preprocess) in enumerate(zip(self.features, self.features_preprocess)):\n",
        "          combined_encoded += self.tokenizer.encode(preprocess(row[feature]), add_special_tokens=False)\n",
        "          if i < len(self.features) - 1:\n",
        "            combined_encoded += [self.tokenizer.sep_token_id]\n",
        "        if len(combined_encoded) > self.max_len:\n",
        "          combined_encoded = combined_encoded[:self.max_len]\n",
        "        attention_mask = [1] * len(combined_encoded)\n",
        "        padded_combined_encoded = combined_encoded + [1] * (self.max_len - len(combined_encoded))\n",
        "        padded_attention_mask = attention_mask + [0] * (self.max_len - len(attention_mask))\n",
        "        input_ids.append(padded_combined_encoded)\n",
        "        attention_masks.append(padded_attention_mask)\n",
        "        targets.append(target)\n",
        "      input_ids = torch.tensor(input_ids)\n",
        "      attention_masks = torch.tensor(attention_masks)\n",
        "      targets = torch.tensor(targets)\n",
        "      return TensorDataset(input_ids, attention_masks, targets)\n",
        "\n",
        "def create_data_loader(dataframe, labels, tokenizer, features, features_preprocess, max_len, batch_size):\n",
        "    dataset = CustomDataset(dataframe, labels, tokenizer, features, features_preprocess, max_len)\n",
        "    return DataLoader(dataset.get_dataset(), batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CW1EcR4jYYMO"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader, data_len, device):\n",
        "  model.eval()\n",
        "  y_preds = np.array([])\n",
        "  y_probs = None\n",
        "  y_trues = np.array([])\n",
        "  losses = []\n",
        "  correct_preds = 0\n",
        "  with torch.no_grad():\n",
        "    for d in tqdm(dataloader):\n",
        "      input_ids = d[0].to(device)\n",
        "      attention_mask = d[1].to(device)\n",
        "      targets = d[2].to(device)\n",
        "      y_trues = np.concatenate((y_trues, targets.cpu().numpy().copy()), axis=0)\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n",
        "      y_prob = softmax(outputs.logits.cpu().tolist(), axis=1)\n",
        "      if y_probs is None:\n",
        "        y_probs = y_prob\n",
        "      else:  \n",
        "        y_probs = np.concatenate((y_probs, y_prob), axis=0)\n",
        "      _, preds = torch.max(outputs.logits, dim=1)\n",
        "      y_preds = np.concatenate((y_preds, preds.cpu().numpy().copy()), axis=0)\n",
        "      loss = outputs.loss\n",
        "      correct_preds += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "  acc = correct_preds / data_len\n",
        "  loss = np.mean(losses)\n",
        "  return y_probs, y_preds, y_trues, acc, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ktko8GuEYaKs"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataloader, eval_dataloader, train_data_len, eval_data_len, device, epochs):\n",
        "  optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct_preds = 0\n",
        "    for d in tqdm(train_dataloader):\n",
        "      input_ids = d[0].to(device)\n",
        "      attention_mask = d[1].to(device)\n",
        "      targets = d[2].to(device)\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n",
        "      _, preds = torch.max(outputs.logits, dim=1)\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      correct_preds += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "    train_acc = correct_preds / train_data_len\n",
        "    train_loss = np.mean(losses)\n",
        "    _, _, _, eval_acc, eval_loss = eval(model, eval_dataloader, eval_data_len, device)\n",
        "    iter_time = time.time() - start\n",
        "    print(f\"epoch {epoch + 1} -- train accuracy: {train_acc}, train loss: {train_loss}, validation accuracy: {eval_acc}, validation loss: {eval_loss}, epoch time: {int(iter_time)} (s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BlZJrD5Ycr_"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_excel(train_path)\n",
        "test_df = pd.read_excel(test_path)\n",
        "valid_df = pd.read_excel(valid_path)\n",
        "labels = {\"quran\": 0, \"bible\": 1, \"mizan\": 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaRCQyGyYff5",
        "outputId": "6b005fc3-baad-475c-8352-83a259e37863"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "100%|██████████| 394/394 [04:45<00:00,  1.38it/s]\n",
            "100%|██████████| 85/85 [00:18<00:00,  4.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 -- train accuracy: 0.8319841027259827, train loss: 0.42535914353988497, validation accuracy: 0.7314814925193787, validation loss: 0.7875272964729982, epoch time: 304 (s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 394/394 [04:44<00:00,  1.38it/s]\n",
            "100%|██████████| 85/85 [00:18<00:00,  4.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2 -- train accuracy: 0.9385713934898376, train loss: 0.17620564140332154, validation accuracy: 0.7818518280982971, validation loss: 0.5977879853809581, epoch time: 303 (s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 394/394 [04:44<00:00,  1.38it/s]\n",
            "100%|██████████| 85/85 [00:18<00:00,  4.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3 -- train accuracy: 0.9618253707885742, train loss: 0.1110631314290659, validation accuracy: 0.7814815044403076, validation loss: 0.690642228196649, epoch time: 303 (s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 394/394 [04:44<00:00,  1.38it/s]\n",
            "100%|██████████| 85/85 [00:18<00:00,  4.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 4 -- train accuracy: 0.9743650555610657, train loss: 0.07591191448949675, validation accuracy: 0.7274073958396912, validation loss: 1.0922722895355785, epoch time: 303 (s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 394/394 [04:44<00:00,  1.38it/s]\n",
            "100%|██████████| 85/85 [00:18<00:00,  4.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 5 -- train accuracy: 0.9786507487297058, train loss: 0.06447205296540302, validation accuracy: 0.6459259390830994, validation loss: 1.8250156795277315, epoch time: 303 (s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 394/394 [04:44<00:00,  1.38it/s]\n",
            "100%|██████████| 85/85 [00:18<00:00,  4.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 6 -- train accuracy: 0.9838888645172119, train loss: 0.05133915505889652, validation accuracy: 0.7411110997200012, validation loss: 1.1660875362508438, epoch time: 303 (s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 394/394 [04:44<00:00,  1.38it/s]\n",
            "100%|██████████| 85/85 [00:18<00:00,  4.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 7 -- train accuracy: 0.986111044883728, train loss: 0.04212768219820895, validation accuracy: 0.7481481432914734, validation loss: 1.1102845977334415, epoch time: 303 (s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 394/394 [04:44<00:00,  1.38it/s]\n",
            "100%|██████████| 85/85 [00:18<00:00,  4.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 8 -- train accuracy: 0.9909523129463196, train loss: 0.02823684218269891, validation accuracy: 0.7503703832626343, validation loss: 1.243247366828077, epoch time: 303 (s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 394/394 [04:44<00:00,  1.38it/s]\n",
            "100%|██████████| 85/85 [00:18<00:00,  4.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 9 -- train accuracy: 0.9898412227630615, train loss: 0.031091301742516255, validation accuracy: 0.7807407379150391, validation loss: 0.9081368716324076, epoch time: 303 (s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 394/394 [04:44<00:00,  1.38it/s]\n",
            "100%|██████████| 85/85 [00:18<00:00,  4.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 10 -- train accuracy: 0.992618978023529, train loss: 0.023373370606119527, validation accuracy: 0.7559259533882141, validation loss: 1.0853802873807794, epoch time: 303 (s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 85/85 [00:18<00:00,  4.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy: 0.7451851963996887, test loss: 1.119846287545036\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       quran       0.74      0.72      0.73       900\n",
            "       bible       0.84      0.55      0.67       900\n",
            "       mizan       0.70      0.97      0.81       900\n",
            "\n",
            "    accuracy                           0.75      2700\n",
            "   macro avg       0.76      0.75      0.74      2700\n",
            "weighted avg       0.76      0.75      0.74      2700\n",
            "\n",
            "0.9211016460905349\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
        "train_loader = create_data_loader(train_df, labels, tokenizer, ['source'], [source_preprocess], 128, 32)\n",
        "eval_loader = create_data_loader(valid_df, labels, tokenizer, ['targets'], [target_preprocess], 128, 32)\n",
        "test_loader = create_data_loader(test_df, labels, tokenizer, ['targets'], [target_preprocess], 128, 32)\n",
        "model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=3)\n",
        "model.to(device)\n",
        "train(model, train_loader, eval_loader, len(train_df), len(valid_df), device, 10)\n",
        "y_probs, y_preds, y_trues, test_acc, test_loss = eval(model, test_loader, len(test_df), device)\n",
        "print(f\"test accuracy: {test_acc}, test loss: {test_loss}\")\n",
        "print()\n",
        "print(classification_report(y_trues.tolist(), y_preds.tolist(), labels=list(labels.values()), target_names=list(labels.keys())))\n",
        "y_trues_onehot = []\n",
        "for y_true in y_trues.tolist():\n",
        "  temp = [0] * len(labels.keys())\n",
        "  temp[int(y_true)] = 1\n",
        "  y_trues_onehot.append(temp)\n",
        "print(roc_auc_score(y_trues_onehot, y_probs.tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Cross-lingual zero-shot transfer learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}